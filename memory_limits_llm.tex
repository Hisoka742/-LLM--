\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage[left=2.5cm, right=1.5cm, top=2cm, bottom=2cm]{geometry}
\usepackage{array}  
\usepackage{float} 
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{indentfirst}
\usepackage[hidelinks]{hyperref}
\usepackage{cite}

\title{Ограничения памяти в LLM-системах: углублённый анализ архитектуры, вычислительной сложности и инженерных компромиссов} \author{} \date{\today}

\begin{document} \maketitle

\begin{abstract} Большие языковые модели (Large Language Models, LLM) стали основой современных систем обработки естественного языка. Несмотря на впечатляющие результаты в генерации текста, рассуждениях и анализе данных, практическое применение LLM ограничено фундаментальными и инженерными аспектами памяти. В данной работе представлен углублённый анализ ограничений длины контекста, вычислительной сложности механизма self-attention, требований к оперативной памяти при инференсе и обучении, а также отсутствия истинной долгосрочной памяти у LLM. Рассматриваются современные подходы к смягчению этих ограничений, включая Retrieval-Augmented Generation (RAG), длинноконтекстные архитектуры и альтернативные механизмы внимания. \end{abstract}

\tableofcontents \newpage

\section{Введение} Большие языковые модели, такие как GPT, LLaMA, Claude и их производные, основаны на архитектуре трансформеров, предложенной в работе Vaswani и др. [1]. Ключевым компонентом этой архитектуры является механизм самовнимания (self-attention), позволяющий моделировать зависимости между всеми токенами входной последовательности.

Несмотря на высокую выразительную способность трансформеров, их практическое использование сопровождается рядом жёстких ограничений, связанных с памятью:

\begin{itemize} \item ограниченная длина входного контекста; \item квадратичная вычислительная и памятьная сложность self-attention; \item высокие требования к VRAM/RAM при инференсе и обучении; \item отсутствие механизма долгосрочной памяти. \end{itemize}

Эти ограничения существенно влияют на применимость LLM для работы с длинными документами, кодовыми базами и диалоговыми агентами [2].

\section{Архитектура трансформеров и память}

\subsection{Общая структура трансформера} Трансформер состоит из стека идентичных слоёв, каждый из которых включает:

\begin{itemize} \item механизм multi-head self-attention; \item позиционно-независимые полносвязные сети; \item остаточные соединения и нормализацию. \end{itemize}

Пусть входная последовательность имеет длину $L$, а размерность скрытого пространства — $d$. Тогда для каждого слоя формируются матрицы запросов $Q$, ключей $K$ и значений $V$ размером $L \times d$.

\subsection{Self-attention и память} Механизм self-attention вычисляется как:

\begin{equation} \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V \end{equation}

Матрица $QK^T$ имеет размер $L \times L$, что приводит к квадратичному росту памяти и времени вычислений:

\begin{equation} O(L^2) \end{equation}

Это фундаментальное свойство архитектуры трансформеров и основная причина ограничения длины контекста [1, 3].

\section{Типы памяти в LLM-системах}

\subsection{Параметрическая память} Параметрическая память — это знания, закодированные в весах модели в процессе обучения. Современные LLM содержат от миллиардов до сотен миллиардов параметров.

Преимущества: \begin{itemize} \item быстрый доступ к знаниям; \item отсутствие необходимости во внешних источниках. \end{itemize}

Недостатки: \begin{itemize} \item устаревание знаний; \item невозможность точечного обновления фактов; \item высокая стоимость переобучения. \end{itemize}

\subsection{Контекстная память} Контекстная память представлена входной последовательностью токенов. Максимальная длина контекста $L$ жёстко ограничена архитектурой и доступной памятью.

Если длина входа превышает $L$, часть информации должна быть отброшена или сжата, что может приводить к потере важных зависимостей [2].

\subsection{Внешняя память} Внешняя память реализуется через базы знаний, поисковые индексы и векторные базы данных. Она активно используется в Retrieval-Augmented Generation (RAG) [4].

\section{Ограничение длины контекста}

\subsection{Теоретические пределы} Квадратичная сложность self-attention приводит к следующим ограничениям:

\begin{itemize} \item рост потребления памяти как $O(L^2)$; \item рост времени инференса как $O(L^2)$; \item ограничение масштабируемости на длинных последовательностях. \end{itemize}

\subsection{Практические значения} Типичные значения максимального контекста:

\begin{itemize} \item 2k–8k токенов — ранние GPT-подобные модели; \item 16k–32k токенов — современные коммерческие модели; \item 100k+ токенов — специализированные длинноконтекстные модели [5]. \end{itemize}

\section{Память при инференсе и обучении}

\subsection{Инференс} Память при инференсе расходуется на:

\begin{itemize} \item хранение весов модели; \item активации слоёв; \item ключи и значения (KV-cache). \end{itemize}

KV-cache растёт линейно с длиной контекста и числом слоёв [6].

\subsection{Обучение} Во время обучения дополнительно хранятся:

\begin{itemize} \item градиенты; \item состояния оптимизатора (например, Adam). \end{itemize}

Это увеличивает потребление памяти в 2–4 раза по сравнению с инференсом [7].

\section{Отсутствие долгосрочной памяти} LLM не имеют истинной долгосрочной памяти:

\begin{itemize} \item контекст теряется после завершения запроса; \item веса модели не обновляются в ходе диалога; \item «память» реализуется на уровне приложения. \end{itemize}

Это требует хранения истории диалога и повторной передачи её в prompt [2].

\section{Инженерные подходы к смягчению ограничений}

\subsection{Сжатие контекста} Используются:

\begin{itemize} \item суммаризация; \item извлечение ключевых фактов; \item удаление нерелевантных фрагментов. \end{itemize}

\subsection{Оконное и разреженное внимание} Модели с локальным вниманием ограничивают область self-attention, снижая сложность до $O(L \cdot w)$, где $w$ — размер окна [3].

\subsection{Retrieval-Augmented Generation} RAG объединяет LLM с внешней памятью:

\begin{enumerate} \item запрос преобразуется в embedding; \item релевантные документы извлекаются из векторной БД; \item они добавляются в prompt модели. \end{enumerate}

Подход описан в работе Lewis и др. [4].

\subsection{Длинноконтекстные архитектуры} Используются:

\begin{itemize} \item sparse attention; \item low-rank аппроксимации; \item рекуррентные механизмы [5]. \end{itemize}

\section{Последствия для практических систем}

\begin{itemize} \item невозможность прямой загрузки больших документов; \item необходимость чанкования и индексации; \item рост латентности; \item увеличение стоимости инференса. \end{itemize}

\section{Будущие направления}

\begin{itemize} \item объединение внешней и внутренней памяти; \item модели с постоянной памятью; \item аппаратные ускорители для attention; \item альтернативные архитектуры (Mamba, RWKV) [8]. \end{itemize}

\section{Заключение} Ограничения памяти в LLM-системах являются следствием фундаментальных свойств архитектуры трансформеров и текущих инженерных компромиссов. Несмотря на развитие длинноконтекстных моделей и внешней памяти, данные ограничения остаются ключевым фактором при проектировании масштабируемых LLM-приложений.

\newpage \section*{Список литературы}

[1] Vaswani A. et al. Attention Is All You Need. NeurIPS, 2017.

[2] Liu J. et al. Lost in the Middle: How Language Models Use Long Contexts. ACL, 2023.

[3] Beltagy I. et al. Longformer: The Long-Document Transformer. arXiv:2004.05150, 2020.

[4] Lewis P. et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. NeurIPS, 2020.

[5] Dao T. et al. FlashAttention: Fast and Memory-Efficient Exact Attention. NeurIPS, 2022.

[6] Kwon W. et al. Efficient Memory Management for Large Language Model Serving. MLSys, 2023.

[7] Rajbhandari S. et al. ZeRO: Memory Optimization Toward Training Trillion Parameter Models. SC, 2020.

[8] Gu A., Dao T. Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv:2312.00752, 2023.

\end{document}
